{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3663b3",
   "metadata": {},
   "source": [
    "## Parameters that may be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Asn_24' # Asn_XX / Asn_XXX, Fc_DAO, Fc_EPO, or NN_modelNSD\n",
    "activ_fun_list = ['relu', 'selu', 'tanh', 'tanhshrink'] # Entries should be in {'relu', 'tanh', 'sigmoid', 'tanhshrink', 'selu'}\n",
    "single_glycan = 'GnGnGnF' # Predict only a single glycan at a time. Must set cutoff = 0 to use this. Set to None when using cutoff\n",
    "nested_validation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97c01d",
   "metadata": {},
   "source": [
    "## Other parameters (that likely shouldn't be changed) + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0 # Ignore all glycans with a train mean < cutoff. Set to 0 when using single_glycan\n",
    "weight_decay = 0 # Should leave equal to 0. weight_decay seldom makes a difference with these data\n",
    "normalize_X = True # Whether to normalize the X data based on the training set mean and stdev\n",
    "# normalize_y does not really work\n",
    "normalize_y = False # Whether to normalize the y data based on the training set mean and stdev\n",
    "\n",
    "# Quick input check - do not modify\n",
    "if cutoff and single_glycan:\n",
    "    raise ValueError('Cutoff must be 0 to use single_glycan. To use cutoff, set single_glycan to None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General & data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import mkdir\n",
    "from os.path import isdir, isfile, dirname\n",
    "from os.path import join as osjoin # join is too generic of a name to be imported directly\n",
    "# Torch & model creation imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "# Training & validation imports\n",
    "from itertools import product\n",
    "# Results & visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "# Convenience imports\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7959b",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1520f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "if exp_name == 'NN_modelNSD':\n",
    "    train_data_X = torch.Tensor(pd.read_csv(osjoin('datasets', 'NN_modelNSD_training-X.csv'), index_col = 0).values)\n",
    "    test_data_X = torch.Tensor(pd.read_csv(osjoin('datasets', 'NN_modelNSD_test-X.csv'), index_col = 0).values)\n",
    "else:\n",
    "    train_data_X = torch.Tensor(pd.read_csv(osjoin('datasets', 'Training-X.csv'), index_col = 0).values)\n",
    "    test_data_X = torch.Tensor(pd.read_csv(osjoin('datasets', 'Test-X.csv'), index_col = 0).values)\n",
    "train_data_y = torch.Tensor(pd.read_csv(osjoin('datasets', f'{exp_name}_training-y.csv'), index_col = 0).values)\n",
    "test_data_y = torch.Tensor(pd.read_csv(osjoin('datasets', f'{exp_name}_test-y.csv'), index_col = 0).values)\n",
    "\n",
    "# Removing glycans based on cutoff\n",
    "if cutoff:\n",
    "    train_mean = train_data_y.mean(axis=0)\n",
    "    above_cutoff = train_mean >= cutoff\n",
    "    train_data_y = train_data_y[:, above_cutoff]\n",
    "    test_data_y = test_data_y[:, above_cutoff]\n",
    "elif single_glycan:\n",
    "    columns = pd.read_csv(osjoin('datasets', f'{exp_name}_training-y.csv'), index_col = 0).columns\n",
    "    glycan_idx = np.argwhere(columns == single_glycan)[0, 0]\n",
    "    train_data_y = train_data_y[:, glycan_idx]\n",
    "    test_data_y = test_data_y[:, glycan_idx]\n",
    "\n",
    "# Nested validation: concatenating the training and test sets\n",
    "if nested_validation:\n",
    "    train_data_X = torch.cat((train_data_X, test_data_X))\n",
    "    train_data_y = torch.cat((train_data_y, test_data_y))\n",
    "\n",
    "# For convenience when declaring MLPs\n",
    "myshape_X = train_data_X.shape[1]\n",
    "if single_glycan:\n",
    "    myshape_y = 1\n",
    "else:\n",
    "    myshape_y = train_data_y.shape[1]\n",
    "# Pre-declaring paths for convenience (to save / load results)\n",
    "if single_glycan:\n",
    "    val_loss_file = f'ANN_{exp_name}_val-loss_{weight_decay:.1e}weight-decay_{single_glycan}.csv'\n",
    "    best_model_file = f'ANN_{exp_name}_{weight_decay:.1e}weight-decay_{single_glycan}_dict.pt'\n",
    "else:\n",
    "    val_loss_file = f'ANN_{exp_name}_val-loss_{weight_decay:.1e}weight-decay_{cutoff}cutoff.csv'\n",
    "    best_model_file = f'ANN_{exp_name}_{weight_decay:.1e}weight-decay_{cutoff}cutoff_dict.pt'\n",
    "# Setting each activ_fun to lowercase for consistency\n",
    "activ_fun_list = [activ_fun.casefold() for activ_fun in activ_fun_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ebf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_working_dir(activ_fun, normalize_X, normalize_y = False, nested_validation = False):\n",
    "    # A helper function to compartimentalize results in labeled folders\n",
    "    if normalize_y:\n",
    "        working_dir = osjoin(f'ANN_normalized-y_results{\"_nested\" * nested_validation}',\n",
    "                             f'ANN_results{\"_nonormalization\" * (not(normalize_X))}_{activ_fun}')\n",
    "    else:\n",
    "        working_dir = osjoin(f'ANN_default-y_results{\"_nested\" * nested_validation}',\n",
    "                             f'ANN_results{\"_nonormalization\" * (not(normalize_X))}_{activ_fun}')\n",
    "    # mkdir is not recursive\n",
    "    first_dir = dirname(working_dir)\n",
    "    if not isdir(first_dir):\n",
    "        mkdir(first_dir)\n",
    "    if not isdir(working_dir):\n",
    "        mkdir(working_dir)\n",
    "    return working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, Xdata, ydata):\n",
    "        self.Xdata = Xdata\n",
    "        self.ydata = ydata\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Xdata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xdata[idx], self.ydata[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7142a",
   "metadata": {},
   "source": [
    "## Model & Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763718b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class SequenceMLP(torch.nn.Module):\n",
    "    def __init__(self, layers, activ_fun = 'relu'):\n",
    "        super(SequenceMLP, self).__init__()\n",
    "        # Setup to convert string (first cell) to activation function\n",
    "        if activ_fun == 'relu':\n",
    "            torch_activ_fun = torch.nn.ReLU()\n",
    "        elif activ_fun == 'tanh':\n",
    "            torch_activ_fun = torch.nn.Tanh()\n",
    "        elif activ_fun == 'sigmoid':\n",
    "            torch_activ_fun = torch.nn.Sigmoid()\n",
    "        elif activ_fun == 'tanhshrink':\n",
    "            torch_activ_fun = torch.nn.Tanhshrink()\n",
    "        elif activ_fun == 'selu':\n",
    "            torch_activ_fun = torch.nn.SELU()\n",
    "        else:\n",
    "            raise ValueError(f'Invalid activ_fun. You passed {activ_fun}')\n",
    "        # Transforming layers list into OrderedDict with layers + activation\n",
    "        mylist = list()\n",
    "        for idx, elem in enumerate(layers):\n",
    "            mylist.append((f'Linear{idx}', torch.nn.Linear(layers[idx][0], layers[idx][1]) ))\n",
    "            if idx < len(layers)-1:\n",
    "                mylist.append((f'{activ_fun}{idx}', torch_activ_fun))\n",
    "        # OrderedDict into NN\n",
    "        self.model = torch.nn.Sequential(OrderedDict(mylist))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b914347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that is called every epoch of training or validation\n",
    "def loop_model(model, optimizer, loader, epoch, evaluation = False):\n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    batch_losses = []\n",
    "\n",
    "    for data in loader:\n",
    "        X, y = data\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(X).squeeze() # TODO: ensure squeeze also works when single_glycan == None\n",
    "        loss = torch.nn.functional.mse_loss(pred, y)\n",
    "        # Backpropagation\n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        batch_losses.append(loss.item()) # Saving losses\n",
    "    return np.array(batch_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyperparameters\n",
    "if cutoff == 0 and not single_glycan: # Predict every glycan at the same time\n",
    "    layers = [\n",
    "        # 1 hidden layer\n",
    "        [(myshape_X, myshape_X*9), (myshape_X*9, myshape_y)],\n",
    "        [(myshape_X, myshape_X*7), (myshape_X*7, myshape_y)],\n",
    "        [(myshape_X, myshape_X*5), (myshape_X*5, myshape_y)],\n",
    "        [(myshape_X, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        # 2 hidden layers\n",
    "        [(myshape_X, myshape_X*9), (myshape_X*9, myshape_X*9), (myshape_X*9, myshape_y)],\n",
    "        [(myshape_X, myshape_X*9), (myshape_X*9, myshape_X*7), (myshape_X*7, myshape_y)],\n",
    "        [(myshape_X, myshape_X*9), (myshape_X*9, myshape_X*5), (myshape_X*5, myshape_y)],\n",
    "        [(myshape_X, myshape_X*9), (myshape_X*9, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*7), (myshape_X*7, myshape_X*7), (myshape_X*7, myshape_y)],\n",
    "        [(myshape_X, myshape_X*7), (myshape_X*7, myshape_X*5), (myshape_X*5, myshape_y)],\n",
    "        [(myshape_X, myshape_X*7), (myshape_X*7, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*5), (myshape_X*5, myshape_y)],\n",
    "        [(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*3), (myshape_X*3, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*3), (myshape_X*3, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_X), (myshape_X, myshape_y)] ]\n",
    "elif cutoff: # Predict a few glycans at the same time\n",
    "    layers = [\n",
    "        # 1 hidden layer\n",
    "        [(myshape_X, myshape_X*4), (myshape_X*4, myshape_y)],\n",
    "        [(myshape_X, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X), (myshape_X, myshape_y)],\n",
    "        # 2 hidden layers\n",
    "        [(myshape_X, myshape_X*4), (myshape_X*4, myshape_X*4), (myshape_X*4, myshape_y)],\n",
    "        [(myshape_X, myshape_X*4), (myshape_X*4, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*3), (myshape_X*3, myshape_X*3), (myshape_X*3, myshape_y)],\n",
    "        [(myshape_X, myshape_X*3), (myshape_X*3, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_X), (myshape_X, myshape_y)],\n",
    "        [(myshape_X, myshape_X), (myshape_X, myshape_X), (myshape_X, myshape_y)] ]\n",
    "else: # Predict a single glycan at a time\n",
    "    layers = [\n",
    "        # 1 hidden layer\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X), (myshape_X, myshape_y)],\n",
    "        [(myshape_X, myshape_X//2), (myshape_X//2, myshape_y)],\n",
    "        # 2 hidden layers\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_X*2), (myshape_X*2, myshape_y)],\n",
    "        [(myshape_X, myshape_X*2), (myshape_X*2, myshape_X), (myshape_X, myshape_y)],\n",
    "        [(myshape_X, myshape_X), (myshape_X, myshape_X), (myshape_X, myshape_y)],\n",
    "        [(myshape_X, myshape_X), (myshape_X, myshape_X//2), (myshape_X//2, myshape_y)] ]\n",
    "lr_vals = [5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4]\n",
    "hyperparam_list = list(product(layers, lr_vals))\n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c2c96",
   "metadata": {},
   "source": [
    "## Training and validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_model(activ_fun, working_dir, val_loss_file, train_idx, val_idx):\n",
    "    \"\"\"\n",
    "    This function runs a cross-validation procedure for each combination of layers + learning rates\n",
    "    Results are saved in a .csv file inside {working_dir}\n",
    "    \"\"\"\n",
    "    # Recording the validation losses\n",
    "    try:\n",
    "        final_val_loss = pd.read_csv(osjoin(f'{working_dir}', f'{val_loss_file}'), index_col = 0)\n",
    "    except FileNotFoundError:\n",
    "        final_val_loss = pd.DataFrame(np.nan, index = lr_vals, columns = [str(elem) for elem in layers])\n",
    "\n",
    "    # Train and validate\n",
    "    print(f'Beginning CV on activation function {activ_fun}')\n",
    "    for cur_idx, cur_hp in enumerate(hyperparam_list):\n",
    "        # We added a new layer configuration to the hyperparameters\n",
    "        if not str(cur_hp[0]) in list(final_val_loss.columns):\n",
    "            final_val_loss.insert(layers.index(cur_hp[0]), str(cur_hp[0]), np.nan) # layers.index to ensure consistent order\n",
    "        # We added a new learning rate to the hyperparameters\n",
    "        elif not cur_hp[1] in final_val_loss.index.to_list():\n",
    "            final_val_loss.loc[cur_hp[1], :] = np.nan\n",
    "\n",
    "        # Run CV only if we do not have validation losses for this set of parameters\n",
    "        if np.isnan( final_val_loss.at[cur_hp[1], str(cur_hp[0])] ):\n",
    "            if not (cur_idx+1) % 5: # Print only every 5 hyperparameters, since they go by pretty quickly\n",
    "                print(f'Beginning hyperparameters {cur_idx+1:2}/{len(hyperparam_list)} for {activ_fun}', end = '\\r')\n",
    "            temp_val_loss = 0\n",
    "            for fold_idx in range(4):\n",
    "                #print(f'Current fold: {fold_idx+1}/4; layers = {cur_hp[0]}, lr = {cur_hp[1]}', end = '\\r') # Folds are too short to bother printing\n",
    "                # Separating the training data for this fold\n",
    "                train_X_fold = train_data_X[train_idx[fold_idx], :]\n",
    "                if single_glycan:\n",
    "                    train_y_fold = train_data_y[train_idx[fold_idx]]\n",
    "                else:\n",
    "                    train_y_fold = train_data_y[train_idx[fold_idx], :]\n",
    "                # Normalizing the training set\n",
    "                if normalize_X:\n",
    "                    mu, std = train_X_fold.mean(), train_X_fold.std()\n",
    "                    train_X_fold = (train_X_fold - mu) / std\n",
    "                if normalize_y:\n",
    "                    mu_y, std_y = train_y_fold.mean(), train_y_fold.std()\n",
    "                    train_y_fold = (train_y_fold - mu_y) / std_y\n",
    "                # Creating the train Dataset / DataLoader\n",
    "                train_dataset_fold = MyDataset(train_X_fold, train_y_fold)\n",
    "                train_loader_fold = DataLoader(train_dataset_fold, batch_size, shuffle = True)\n",
    "                \n",
    "                # Separating the validation data for this fold\n",
    "                val_X_fold = train_data_X[val_idx[fold_idx], :]\n",
    "                if single_glycan:\n",
    "                    val_y_fold = train_data_y[val_idx[fold_idx]]\n",
    "                else:\n",
    "                    val_y_fold = train_data_y[val_idx[fold_idx], :]\n",
    "                # Normalizing the validation set with training set mu and std\n",
    "                if normalize_X:\n",
    "                    val_X_fold = (val_X_fold - mu) / std\n",
    "                if normalize_y:\n",
    "                    val_y_fold = (val_y_fold - mu_y) / std_y\n",
    "                # Creating the val Dataset / DataLoader\n",
    "                val_dataset_fold = MyDataset(val_X_fold, val_y_fold)\n",
    "                val_loader_fold = DataLoader(val_dataset_fold, batch_size, shuffle = True)\n",
    "\n",
    "                # Declaring the model and optimizer\n",
    "                model = SequenceMLP(cur_hp[0], activ_fun).cuda()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = cur_hp[1], weight_decay = weight_decay)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = 20, verbose = False, min_lr = 1e-5)\n",
    "                # Train and validate\n",
    "                for epoch in range(200):\n",
    "                    train_loss = loop_model(model, optimizer, train_loader_fold, epoch)\n",
    "                    val_loss = loop_model(model, optimizer, val_loader_fold, epoch, evaluation = True)\n",
    "                    scheduler.step(val_loss)\n",
    "                # Recording the validation loss for this fold\n",
    "                temp_val_loss += val_loss / 4 # Divided by 4 because there are 4 folds\n",
    "\n",
    "            # Saving the average validation loss after CV\n",
    "            final_val_loss.at[cur_hp[1], str(cur_hp[0])] = temp_val_loss\n",
    "            if not nested_validation: # Nested validation requires a bunch of inputs to the same file - I haven't implemented a good way to save the intermetiate data\n",
    "                final_val_loss.to_csv(osjoin(f'{working_dir}', f'{val_loss_file}'))\n",
    "    return final_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ffb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_val_loss_list = np.empty_like(activ_fun_list, dtype = object) # This will hold multiple DataFrames, one for each activation fun type\n",
    "for idx, activ_fun in enumerate(activ_fun_list):\n",
    "    if nested_validation and exp_name == 'NN_modelNSD': # TODO: can likely remove this if-statement from the for loop\n",
    "        train_idx = np.zeros((5, 4), dtype = object) # 5 nested validation sets, 4 cross validation sets per nested set\n",
    "        val_idx = np.zeros_like(train_idx)\n",
    "        # To assist in the setup by marking which set will be the test set\n",
    "        test_set_list = [set(elem) for elem in [range(0, 4), range(4, 7), range(7, 11), range(11, 15), range(15, 19)]]\n",
    "        for nested_idx in range(len(test_set_list)):\n",
    "            list_for_cv = test_set_list[:nested_idx] + test_set_list[nested_idx+1:] # Excluding one set for testing\n",
    "            train_idx[nested_idx, :] = [list(set(range(19)) - test_set_list[nested_idx] - list_for_cv[fold_idx]) for fold_idx in range(4)]\n",
    "            val_idx[nested_idx, :] = [list(elem) for elem in list_for_cv]\n",
    "    elif nested_validation: # Asn_ and Fc_DAO/Fc_EPO with nested validation\n",
    "        train_idx = np.zeros((5, 4), dtype = object) # 5 nested validation sets, 4 cross validation sets per nested set\n",
    "        val_idx = np.zeros_like(train_idx)\n",
    "        # First set: equal to the cross-validation idxs\n",
    "        train_idx[0, :] = [list( set(range(12)) - set(range(3*fold_idx, 3*(fold_idx+1))) ) for fold_idx in range(4)]\n",
    "        val_idx[0, :] = [range(3*fold_idx, 3*(fold_idx+1)) for fold_idx in range(4)]\n",
    "        # Other sets: need to include the last row_idxs in the training / validation (to change the test set)\n",
    "        for nested_idx in range(1, train_idx.shape[0]):\n",
    "            temp = list( set(range(15)) - set(range(3*(nested_idx-1), 3*nested_idx)) ) # Excluding the current test set\n",
    "            train_idx[nested_idx, :] = [temp[:3*fold_idx] + temp[3*(fold_idx+1):] for fold_idx in range(4)]\n",
    "            val_idx[nested_idx, :] = [temp[3*fold_idx:3*(fold_idx+1)] for fold_idx in range(4)]\n",
    "    elif exp_name == 'NN_modelNSD':\n",
    "        # No convenient way to setup the idx\n",
    "        train_idx = [list(range(4, 15)), list(range(0, 4)) + list(range(7, 15)), list(range(0, 7)) + list(range(11, 15)), list(range(0, 12))]\n",
    "        val_idx = [list( set(range(15)) - set(train_idx[fold_idx]) ) for fold_idx in range(4)]\n",
    "    else: # Asn_ and Fc_DAO/Fc_EPO\n",
    "        train_idx = [list( set(range(12)) - set(range(3*fold_idx, 3*(fold_idx+1))) ) for fold_idx in range(4)]\n",
    "        val_idx = [range(3*fold_idx, 3*(fold_idx+1)) for fold_idx in range(4)]\n",
    "\n",
    "    working_dir = generate_working_dir(activ_fun, normalize_X, normalize_y, nested_validation) # Results folder\n",
    "    # Running the CV (no nested validation) / groups of CVs (nested validation)\n",
    "    if nested_validation:\n",
    "        print(f'Nested validation: beginning CV loop 1/{train_idx.shape[0]}')\n",
    "        final_val_loss_list[idx] = CV_model(activ_fun, working_dir, val_loss_file, train_idx[0, :], val_idx[0, :])\n",
    "        if not isfile(osjoin(f'{working_dir}', f'{val_loss_file}')): # Avoid replacing (and thus incorrectly increasing the losses every run)\n",
    "            for cv_idx in range(1, train_idx.shape[0]):\n",
    "                print(f'Nested validation: beginning CV loop {cv_idx+1}/{train_idx.shape[0]}')\n",
    "                final_val_loss_list[idx] += CV_model(activ_fun, working_dir, val_loss_file, train_idx[cv_idx, :], val_idx[cv_idx, :])\n",
    "            # Saving the results after all nested validation runs (for non-nested, save occurs within the CV_model function)\n",
    "            final_val_loss_list[idx].to_csv(osjoin(f'{working_dir}', f'{val_loss_file}'))\n",
    "    else:\n",
    "        final_val_loss_list[idx] = CV_model(activ_fun, working_dir, val_loss_file, train_idx, val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a8d0d",
   "metadata": {},
   "source": [
    "## Final Evaluation - Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ed9a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_final_evaluation(model, working_dir, activ_fun):\n",
    "    model.eval()\n",
    "\n",
    "    # Train loss\n",
    "    if single_glycan:\n",
    "        train_pred = torch.empty(len(train_loader.dataset))\n",
    "    else:\n",
    "        train_pred = torch.empty((len(train_loader.dataset), myshape_y))\n",
    "    train_y = torch.empty_like(train_pred)\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        X, y = data\n",
    "        X = X.cuda()\n",
    "        pred = model(X).cpu().detach().squeeze()\n",
    "        if single_glycan:\n",
    "            train_pred[idx*batch_size:(idx*batch_size)+len(pred)] = pred\n",
    "            train_y[idx*batch_size:(idx*batch_size)+len(y)] = y\n",
    "        else:\n",
    "            train_pred[idx*batch_size:(idx*batch_size)+len(pred), :] = pred\n",
    "            train_y[idx*batch_size:(idx*batch_size)+len(y), :] = y\n",
    "    #train_loss = torch.nn.functional.mse_loss(train_pred, train_y)\n",
    "    #print(f'The train loss was {train_loss.item():.2e}')\n",
    "    # Train predictions plot\n",
    "    if single_glycan:\n",
    "        print(f'Predicted training mean {train_pred.mean():.3f} ±{train_pred.std():.3f}, real training mean {train_y.mean():.3f}')\n",
    "        if normalize_y:\n",
    "            print(f'\\tDenormalized: predicted {train_pred.mean()*std_y + mu_y:.3f}, real {train_y.mean()*std_y + mu_y:.3f}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize = (8,8))\n",
    "        ax.set_title('Train Predictions - mean')\n",
    "        plt.scatter(train_y.mean(axis = 0), train_pred.mean(axis = 0))\n",
    "        ax.set_xlabel('Real Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    ax.set_title(f'{activ_fun}: Train Predictions')\n",
    "    plt.scatter(train_y, train_pred)\n",
    "    ax.set_xlabel('Real Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    axis_min_lim = torch.minimum(train_pred.min(), train_y.min())\n",
    "    axis_min_lim = np.minimum(axis_min_lim, 0) # If all values are positive, start from 0\n",
    "    axis_max_lim = torch.maximum(train_pred.max(), train_y.max())\n",
    "    ax.set_xlim(axis_min_lim - 0.01, axis_max_lim + 0.01)\n",
    "    ax.set_ylim(axis_min_lim - 0.01, axis_max_lim + 0.01)\n",
    "    # Perfect prediction line (just a 45° line)\n",
    "    plt.plot([axis_min_lim, axis_max_lim], [axis_min_lim, axis_max_lim], color = 'k', linestyle = '--', linewidth = 1)\n",
    "    # 10% relative error lines\n",
    "    plt.plot([axis_min_lim, axis_max_lim], [1.1*axis_min_lim, 1.1*axis_max_lim], color = 'r', linestyle = '--', linewidth = 1)\n",
    "    plt.plot([axis_min_lim, axis_max_lim], [0.9*axis_min_lim, 0.9*axis_max_lim], color = 'r', linestyle = '--', linewidth = 1)\n",
    "    # Test loss\n",
    "    if single_glycan:\n",
    "        test_pred = torch.empty(len(test_loader.dataset))\n",
    "    else:\n",
    "        test_pred = torch.empty((len(test_loader.dataset), myshape_y))\n",
    "    test_y = torch.empty_like(test_pred)\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        X, y = data\n",
    "        X = X.cuda()\n",
    "        pred = model(X).cpu().detach().squeeze()\n",
    "        if single_glycan:\n",
    "            test_pred[idx*batch_size:(idx*batch_size)+len(pred)] = pred\n",
    "            test_y[idx*batch_size:(idx*batch_size)+len(y)] = y\n",
    "        else:\n",
    "            test_pred[idx*batch_size:(idx*batch_size)+len(pred), :] = pred\n",
    "            test_y[idx*batch_size:(idx*batch_size)+len(y), :] = y\n",
    "    #test_loss = torch.nn.functional.mse_loss(test_pred, test_y)\n",
    "    #print(f'The test loss was {test_loss:.2e}')\n",
    "    # Test predictions plot\n",
    "    if single_glycan:\n",
    "        print(f'Predicted test mean {test_pred.mean():.3f} ±{test_pred.std():.3f}; real test mean {test_y.mean():.3f}')\n",
    "        if normalize_y:\n",
    "            print(f'\\tDenormalized: predicted {test_pred.mean()*std_y + mu_y:.3f}, real {test_y.mean()*std_y + mu_y:.3f}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize = (8,8))\n",
    "        ax.set_title('Test Predictions - mean')\n",
    "        plt.scatter(test_y.mean(axis = 0), test_pred.mean(axis = 0))\n",
    "        ax.set_xlabel('Real Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        # Changing the axis limits for easier visualization\n",
    "        axis_lim = torch.maximum(test_pred.mean(axis=0).max(), test_y.mean(axis=0).max())\n",
    "        ax.set_xlim(-0.01, axis_lim + 0.01)\n",
    "        ax.set_ylim(-0.01, axis_lim + 0.01)\n",
    "        # Saving the test mean predictions for comparison\n",
    "        pd.DataFrame(test_pred.mean(axis = 0)).T.to_csv(osjoin(f'{working_dir}', f'temp_{exp_name}.csv'), header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7126b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Removing the test set from the training data for the final evaluation\n",
    "if nested_validation and exp_name == 'NN_modelNSD' and train_data_X.shape[0] == 19:\n",
    "    train_data_X = train_data_X[:-4]\n",
    "    train_data_y = train_data_y[:-4]\n",
    "elif nested_validation and exp_name != 'NN_modelNSD' and train_data_X.shape[0] == 15:\n",
    "    train_data_X = train_data_X[:-3]\n",
    "    train_data_y = train_data_y[:-3]\n",
    "\n",
    "# Normalizing the training set\n",
    "if normalize_X:\n",
    "    mu, std = train_data_X.mean(), train_data_X.std()\n",
    "    train_data_X = (train_data_X - mu) / std\n",
    "if normalize_y:\n",
    "    mu_y, std_y = train_data_y.mean(), train_data_y.std()\n",
    "    train_data_y = (train_data_y - mu_y) / std_y\n",
    "# Creating the full training Dataset / DataLoader\n",
    "train_dataset = MyDataset(train_data_X, train_data_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "# Normalizing the test set with training set mu and std\n",
    "if normalize_X:\n",
    "    test_data_X = (test_data_X - mu) / std\n",
    "if normalize_y:\n",
    "    test_data_y = (test_data_y - mu_y) / std_y\n",
    "# Creating the test Dataset / DataLoader\n",
    "test_dataset = MyDataset(test_data_X, test_data_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle = True)\n",
    "\n",
    "print(f'Current glycan: {exp_name}-{single_glycan}')\n",
    "for final_val_loss, activ_fun in zip(final_val_loss_list, activ_fun_list):\n",
    "    working_dir = generate_working_dir(activ_fun, normalize_X, normalize_y, nested_validation) # Results folder\n",
    "    # Finding the best hyperparameters\n",
    "    best_idx = np.argmin(final_val_loss.values.T) # .T to ensure proper ordering, as argmin flattens the array\n",
    "    # Re-declaring the model\n",
    "    model = SequenceMLP(hyperparam_list[best_idx][0], activ_fun).cuda()\n",
    "\n",
    "    # Checking if we already retrained this model\n",
    "    try:\n",
    "        mydict = torch.load(osjoin(f'{working_dir}', f'{best_model_file}'))\n",
    "        model.load_state_dict(mydict)\n",
    "    except FileNotFoundError: # Retraining the model with the full training set\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = hyperparam_list[best_idx][1], weight_decay = weight_decay)\n",
    "        # Retrain\n",
    "        for epoch in range(200):\n",
    "            train_loss = loop_model(model, optimizer, train_loader, epoch)\n",
    "        # Save the retrained model\n",
    "        torch.save(model.state_dict(), osjoin(f'{working_dir}', f'{best_model_file}'))\n",
    "    \n",
    "    # CV Data\n",
    "    print(f'Final results for {activ_fun} and weight decay {weight_decay:.1e}')\n",
    "    print(f'Best hyperparameters: {hyperparam_list[best_idx]}')\n",
    "    print(f'CV Loss: {final_val_loss.at[ hyperparam_list[best_idx][1], str(hyperparam_list[best_idx][0]) ]:.5e}')\n",
    "    run_final_evaluation(model, working_dir, activ_fun)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
